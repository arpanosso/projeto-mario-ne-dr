---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, warning = FALSE, erro=FALSE, message = FALSE,
  comment = "#>"
)
```

## APRENDIZADO DE MÁQUINA PARA PREDIÇÃO DA VARIABILIDADE ESPAÇO-TEMPORAL DA PRODUÇÃO DE CANA-DE-AÇÚCAR

## Carregando pacotes

```{r}
library(tidyverse)
library(ggridges)
library(vegan)
library(corrplot)
library(sp)
library(gstat)
source("R/my-functions.R")
theme_set(theme_bw())
```


## Lendo o banco de dados

```{r}
data_set <- read_rds("data/sugarcane-soil.rds")  |> 
  rename_with(~ str_replace(., "_2$", ""), ends_with("_2")) |>  
    rename(TCH = tch_real,
           Ca = ca,
           `m%` = m,
           SB = sb,
           CTC = ctc,
           P = p_resina,
           `H+Al` = h_al,
           `V%` = v,
           K = k,
           Mg = mg,
           MO = mo,
           pH = ph_cacl2,
           S = s) 
glimpse(data_set)
```

## 1. Análise Estatística Exploratória

Tabela com a estatística descritiva por unidade (CAT e POT) para as variáveis numéricas (n, min, q1, mediana, média, q3, max, dp, epm, cv, cassimetria, ccurtose) para as variáveis `tch_real` até `s_2`...nos diferentes tempos, pra caracterizarmos a variabilidade total do conjunto de dados

criando a função para a análise exploratória dos dados

```{r}
estat_desc <- function(x){
  set.seed(1235)
  xs <- sample(x,500)
  normal <- nortest::lillie.test(xs)
  log_norm <- nortest::lillie.test(log(xs+1))
  n <- length(x)
  m <- mean(x,na.rm = TRUE)
  md <- median(x)
  mini <- min(x,na.rm = TRUE)
  q1 <- quantile(x,.25)
  q3 <- quantile(x,.75)
  maxi <- max(x,na.rm = TRUE)
  dp <- sd(x,na.rm = TRUE)
  epm <- dp/sqrt(n)
  cv <- 100*dp/m
  ass <- agricolae::skewness(x)
  curt <- agricolae::kurtosis(x)
  c(n,mini,q1,md,m,q3,maxi,dp,epm,cv,ass,curt,normal$p.value,log_norm$p.value)
}
estat_names <- c("n","Min","Q1","Med","Média","Q3",
                 "Max","DP","EP","CV","Skn","Krt","norm","log_norm")
```


Tabela de estatística descritiva presente no arquivo do excel na pasta `output`.

```{r}
data_set |> 
  group_by(ano,unidade) |> 
  reframe(
    across(.cols = TCH:S,
           .fns = estat_desc,
           .names = "{.col}")
  )|> 
  add_column(estatistica = rep(estat_names,2*3)) |> 
  relocate(estatistica) |> 
  writexl::write_xlsx("output/estat-desc.xlsx")
```

Boxplot para comparação

```{r}
var_names <- data_set |> select(TCH:S) |> names()
map(var_names, ~{
    x<-data_set |> pull(!!sym(.x))
  q3 <- quantile(x,0.99)
  data_set |> 
    filter(!!sym(.x) <= q3) |>  
  ggplot(aes(x=as_factor(ano),y=!!sym(.x),fill=unidade)) +
  geom_boxplot() +
  scale_fill_manual(values=c("#009E73", "#D55E00")) +
  labs(x="Ano", y=.x)
})
```
```{r}
map(var_names, ~{
  x<-data_set |> pull(!!sym(.x))
  q3 <- quantile(x,0.99)
  data_set |> 
    filter(!!sym(.x) <= q3) |> 
    ggplot(aes(x=!!sym(.x),fill=unidade)) +
    geom_histogram(color="black") +
    scale_fill_manual(values=c("#009E73", "#D55E00")) +
    labs(x="Ano", y=.x)+
    facet_wrap(~ano,ncol=2)
})
```
```{r}
map(var_names, ~{
  x<-data_set |> pull(!!sym(.x))
  q3 <- quantile(x,0.99)
  data_set |> 
    filter(!!sym(.x) <= q3) |> 
    ggplot(aes(y=as_factor(ano))) +
    geom_density_ridges(rel_min_height = 0.03,
                        aes(x=!!sym(.x), fill=unidade),
                        alpha = .6, color = "black"
    ) +
    scale_fill_cyclical(values = c("#ff8080","#238B45"),
                        name = "classe", guide = "legend") +
    labs(y="Ano", x=.x)+
    theme_ridges()
})
```




## 2. Matriz de correlação linear (corplot) fazer por ano (2016, 2017 e 2018) e por UNIDADE


Para CAT
```{r}
map(2016:2018, ~{data_set |> filter(unidade == "CAT") |> 
  filter(ano == .x) |> 
  select(TCH:S) |> 
  cor() |> 
  corrplot::corrplot( method = "color",
         outline = T,,
         addgrid.col = "darkgray",cl.pos = "r", tl.col = "black",
         tl.cex = 1, cl.cex = 1, type = "upper", bg="azure2",
         diag = FALSE,
         addCoef.col = "black",
         cl.ratio = 0.2,
         cl.length = 5,
         number.cex = 0.8)}) 
```

Para POT
```{r}
map(2016:2018, ~{data_set |> filter(unidade == "POT") |> 
  filter(ano == .x) |> 
  select(TCH:S) |> 
  cor() |> 
  corrplot::corrplot( method = "color",
         outline = T,,
         addgrid.col = "darkgray",cl.pos = "r", tl.col = "black",
         tl.cex = 1, cl.cex = 1, type = "upper", bg="azure2",
         diag = FALSE,
         addCoef.col = "black",
         cl.ratio = 0.2,
         cl.length = 5,
         number.cex = 0.8)}) 
```

### 2.1 Análise de agrupamento independente do ano

```{r}
da <- data_set |> 
  group_by(x,y) |> 
  summarise(
    across(.cols = TCH:S,
           .fns = mean,
           .names = "{.col}"),
    .groups = "drop"
  ) |> select(-x,-y)

da_pad <- decostand(da, 
                  method = "standardize",
                  na.rm=TRUE)
da_pad_euc <- vegdist(da_pad, "euclidean") 
da_pad_euc_ward<-hclust(da_pad_euc, method="ward.D")
plot(da_pad_euc_ward, 
     ylab="Distância Euclidiana",
     xlab="Acessos", hang=-1,
     col="blue", las=1,
     cex=.6,lwd=1.5);box()
grupo <- cutree(da_pad_euc_ward,3)
```



### 2.2 análise de componentes principais 

```{r}
print("======== Análise de Componentes Principais ========== ")
pca <-  prcomp(da_pad,scale.=TRUE)
# Autovalores
eig<-pca$sdev^2
print("==== Autovalores ====")
print(round(eig,3))
print("==== % da variância explicada ====")
ve<-eig/sum(eig)
print(round(ve,4))
print("==== % da variância explicada acumulada ====")
print(round(cumsum(ve),4)*100)
print("==== Poder Discriminante ====")
mcor<-cor(da_pad,pca$x)
corrplot(mcor)
print("==== screeplot ====")
screeplot(pca)
abline(h=1)
```

```{r}
pc1V<-cor(da_pad,pca$x)[,1]/sd(cor(da_pad,pca$x)[,1])
pc2V<-cor(da_pad,pca$x)[,2]/sd(cor(da_pad,pca$x)[,2])
pc3V<-cor(da_pad,pca$x)[,3]/sd(cor(da_pad,pca$x)[,3])
pc1c<-pca$x[,1]/sd(pca$x[,1])
pc2c<-pca$x[,2]/sd(pca$x[,2])
pc3c<-pca$x[,3]/sd(pca$x[,3])
nv<-ncol(da) # número de variáveis utilizadas na análise
```

```{r}
# gráfico biplot
bip<-data.frame(pc1c,pc2c,pc3c,grupo)
texto <- data.frame(
  x = pc1V,
  y = pc2V,
  z = pc3V,
  label = names(da)
)

bi_plot <- bip |> 
  ggplot(aes(x=pc1c,y=pc2c,color = as_factor(grupo)))+
  geom_point() + 
  theme_minimal() +
   scale_shape_manual(values=16:18)+
  scale_color_manual(values=c("#009E73", "#999999","#D55E00")) +
  annotate(geom="text", x=pc1V, y=pc2V, label=names(pc1V),
              color="black",font=3)+
  geom_vline(aes(xintercept=0),
             color="black", size=1)+
  geom_hline(aes(yintercept=0),
             color="black", size=1)+
  annotate(geom="segment",
           x=rep(0,length(da)),
           xend=texto$x,
           y=rep(0,length(da)),
           yend=texto$y,color="black",lwd=.5)+
  geom_label(data=texto,aes(x=x,y=y,label=label),
             color="black",angle=0,fontface="bold",size=4,fill="white")+
  labs(x=paste("CP1 (",round(100*ve[1],2),"%)",sep=""),
       y=paste("CP2 (",round(100*ve[2],2),"%)",sep=""),
       color="",shape="")+
  theme(legend.position = "top")
bi_plot +
  coord_cartesian(
    xlim = c(-4,3),
    ylim = c(-3,8)
  )
```

```{r}
# gráfico biplot
bi_plot_2 <- bip |> 
  ggplot(aes(x=pc1c,y=pc3c,color = as_factor(grupo)))+
  geom_point() + 
  theme_minimal() +
  scale_shape_manual(values=16:18)+
  scale_color_manual(values=c("#009E73", "#999999","#D55E00")) +
  annotate(geom="text", x=pc1V, y=pc3V, label=names(pc1V),
              color="black",font=3)+
  geom_vline(aes(xintercept=0),
             color="black", size=1)+
  geom_hline(aes(yintercept=0),
             color="black", size=1)+
  annotate(geom="segment",
           x=rep(0,length(da)),
           xend=texto$x,
           y=rep(0,length(da)),
           yend=texto$z,color="black",lwd=.5)+
  geom_label(data=texto,aes(x=x,y=z,label=label),
             color="black",angle=0,fontface="bold",size=4,fill="white")+
  labs(x=paste("CP1 (",round(100*ve[1],2),"%)",sep=""),
       y=paste("CP3 (",round(100*ve[3],2),"%)",sep=""),
       color="",shape="")+
  theme(legend.position = "top")
bi_plot_2 +
  coord_cartesian(
    xlim = c(-4,3),
    ylim = c(-3,8)
  )
```
```{r}
print("==== Tabela da correlação dos atributos com cada PC ====")
    ck<-sum(pca$sdev^2>=0.98)
    tabelapca<-vector()
    for( l in 1:ck) tabelapca<-cbind(tabelapca,mcor[,l])
    colnames(tabelapca)<-paste(rep(c("PC"),ck),1:ck,sep="")
    pcat<-round(tabelapca,3)
    tabelapca<-tabelapca[order(abs(tabelapca[,1])),]
    print(tabelapca)
writexl::write_xlsx(data.frame(tabelapca) |> 
                      add_column(nome = row.names(tabelapca)) |> 
                      relocate(nome), "output/pca-table.xlsx")
```


```{r}
 data_set |> 
  group_by(x,y) |> 
  summarise(
    across(.cols = TCH:S,
           .fns = mean,
           .names = "{.col}"),
    .groups = "drop"
  ) |> 
  add_column(grupo) |> 
  ggplot(aes(x=x,y=y,color=as_factor(grupo))) +
  geom_point() +
  scale_shape_manual(values=16:18)+
  scale_color_manual(values=c("#009E73", "#999999","#D55E00")) 
```
```{r}
data_set |> 
  group_by(unidade,x,y) |> 
  summarise(
    across(.cols = TCH:S,
           .fns = mean,
           .names = "{.col}"),
    .groups = "drop"
  ) |> 
  filter(unidade == "POT") |>
  ggplot(aes(x,y)) +
  geom_point()
```
## Criando o contorno e POT

```{r}
contorno_pot <- data_set |> 
  group_by(unidade,x,y) |> 
  summarise(
    n=n(),
    .groups = "drop"
  ) |> 
  filter(unidade == "POT") |> 
  mutate(
    x1=mean(x),
    y1=mean(y),
    ang_rad = round(atan2(y-y1,x-x1),1),
    distancia = sqrt((x - x1)^2 + (y - y1)^2),
  ) |> 
  group_by(ang_rad) |> 
  mutate(
    max_dist = max(distancia)
  ) |> 
  ungroup() |> 
  mutate(flag_dist =distancia == max_dist) |> 
  filter(flag_dist)  |> 
  arrange(ang_rad) |> 
  select(x,y) |>   
  as.data.frame()
contorno_pot <- rbind(contorno_pot, contorno_pot[1,]) 

ggplot(contorno_pot, aes(x = x, y = y)) +
  geom_polygon(fill = "lightblue", color = "black") +
  coord_equal() +  # Mantém proporção real dos eixos
  theme_minimal()
```
## Criando o contorno e CAT

```{r}
contorno_cat <- data_set |> 
  group_by(unidade,x,y) |> 
  summarise(
    n=n(),
    .groups = "drop"
  ) |> 
  filter(unidade == "CAT") |> 
  mutate(
    x1=mean(x),
    y1=mean(y),
    ang_rad = round(atan2(y-y1,x-x1),1),
    distancia = sqrt((x - x1)^2 + (y - y1)^2),
  ) |> 
  group_by(ang_rad) |> 
  mutate(
    max_dist = max(distancia)
  ) |> 
  ungroup() |> 
  mutate(flag_dist =distancia == max_dist) |> 
  filter(flag_dist)  |> 
  arrange(ang_rad) |> 
  select(x,y) |>   
  as.data.frame()
contorno_cat <- rbind(contorno_cat, contorno_cat[1,]) 

ggplot(contorno_cat, aes(x = x, y = y)) +
  geom_polygon(fill = "pink", color = "black") +
  coord_equal() +  # Mantém proporção real dos eixos
  theme_minimal()
```
```{r}
contorno_total <- rbind(contorno_cat |> 
                    mutate(unidade="CAT"),
                  contorno_pot |> 
                    mutate(unidade="POT"))
contorno_total |> ggplot(aes(x = x, y = y, group = unidade, fill = unidade)) +
  geom_polygon(color = "black", alpha = 0.5) +
  coord_equal() +
  theme_minimal()+
  geom_point(data = data_set |> 
               filter(ano == 2018),
             aes(x,y,color=unidade))
```


## Comparação variáveis originais e log transformadas

```{r}
map(var_names, ~{
  # x<-data_set |> pull(!!sym(.x))
  # x<-log(x+1)
  # q3 <- quantile(x,0.99)
  new_var <- paste0("log(", .x,"+1)")
  data_set |> 
    mutate(!!new_var := log(!!sym(.x) +1)) |> 
    select(!!sym(.x), !!sym(new_var)) |> 
    pivot_longer(cols = c(!!sym(.x), !!sym(new_var)),
                 names_to = "variavel",
                 values_to = "valor") |> 
    ggplot(aes(valor,fill = variavel)) +
    geom_histogram(color="black") +
    # labs(x="Ano", y=paste0("log(",.x,"+1)")) +
      facet_wrap(~variavel, scale="free") +
    scale_fill_brewer(palette = "Dark2")
})
```

## 3. Análise e modelagem geoestatística (2016, 2017 e 2018) por UNIDADE.

### Separando o banco de dados para unidade CAT e transformar se necessário

```{r}
ano_analise <- 2016
variavel <- "TCH"
unidade_analise <- "CAT"
data_set_aux <- data_set |> 
  filter(
    ano == ano_analise,
    unidade == unidade_analise) |> 
  select(x,y,variavel) |> 
  rename(z = !!sym(variavel)) |> 
  group_by(x,y) |> 
  summarise(
    z = mean(z,na.rm = TRUE),
    .groups = "drop"
  )
glimpse(data_set_aux)
writexl::write_xlsx(data_set_aux,"data_set_aux.xlsx")
```

### Transformar se necessário

```{r}
coordinates(data_set_aux) = ~ x + y  
form <- z ~ 1 # fórmula da função variogram
```


Construir o semivariograma experimental

```{r}
vari_exp <- variogram(form, data = data_set_aux,
                      cressie = FALSE,
                      cutoff = 0.10, # distância máxima do semivariograma
                      width = .008) # distância entre pontos
vari_exp  |>  
 ggplot(aes(x=dist, y=gamma)) +
 geom_point() +
 labs(x="lag (º)",
      y=expression(paste(gamma,"(h)")))
```
## Escolher o melhor modelo

```{r}
patamar=600
alcance=0.02
epepita=0
modelo_1 <- fit.variogram(vari_exp,vgm(patamar,"Sph",alcance,epepita))
modelo_2 <- fit.variogram(vari_exp,vgm(patamar,"Exp",alcance,epepita))
modelo_3 <- fit.variogram(vari_exp,vgm(patamar,"Gau",alcance,epepita))
plot_my_models(modelo_1,modelo_2,modelo_3)
```
### Validação Cruzada
```{r}
my_cross_validation(data_set_aux,form,modelo_1,modelo_2,modelo_3)
```
### Escolha do melhor modelo

O melhor modelo é aquele que apresenta um coeficiente de regressão o mais próximo de $1$ e o intercepto o mais próximo de $0$, com o menor RMSE possível.

#### Definido o melhor modelo, precisamos guardar os valores.

```{r}
modelo <- modelo_2 ## sempre modificar
model <- modelo |> slice(2) |> pull(model)
rss <- round(attr(modelo, "SSErr"),4)
c0 <- round(modelo$psill[[1]],4)
c0_c1 <- round(sum(modelo$psill),4)
a <- ifelse(model == "Gau", round(modelo$range[[2]]*(3^.5),2),
            ifelse(model == "Exp",round(3*modelo$range[[2]],2),
            round(modelo$range[[2]],2)))
r2 <- vari_exp |> add_column( model = model, a=a, c0 = c0,
                                  c0_c1 = c0_c1) |>
    mutate(
      gamma_m = ifelse(model == "Sph",
        ifelse(dist <= a, c0 + (c0_c1 - c0) * (3/2 * (dist/a) - 1/2 * (dist/a)^3),c0_c1), ifelse(model == "Exp", c0 + (c0_c1-c0)*(1-exp(-3*(dist/a))),c0 + (c0_c1-c0)*(1-exp(-3*(dist/a)^2)))),
      residuo_total = (gamma-mean(gamma))^2,
      residuo_mod = (gamma - gamma_m)^2
    ) |>
    summarise(
      r2=(sum(residuo_total) - sum(residuo_mod))/sum(residuo_total)
    ) |> pull(r2)

tibble(
  ano_analise, unidade_analise, variavel, model, c0, c0_c1, a, rss, r2
) |> mutate(gde = c0/c0_c1, .after = "a") |>
  write_csv(paste0("output/best-fit/",ano_analise,"-",
                   unidade_analise,"-",variavel,".csv"))

ls_csv <- list.files("output/best-fit/",full.names = TRUE,pattern = ".csv")
map_df(ls_csv, read_csv) |>
  writexl::write_xlsx("output/semivariogram-models.xlsx")
png(filename = paste0("output/semivariogram-img/semivar-",
                      ano_analise,"-",
                   unidade_analise,"-",variavel,".png"),
    width = 800, height = 600)
plot(vari_exp,model=modelo,cex.lab=2, col=1,pl=F,pch=16,cex=2.2,ylab=list("Semivariância",cex=2.3),xlab=list("Distância de Separação h (m)",cex=2.3,cex.axis=4))
dev.off()
```

### Krigragem ordinária (KO)
Utilizando o algorítmo de KO, vamos estimar xco2 nos locais não amostrados.


```{r}
x<-data_set_aux$x
y<-data_set_aux$y
dis <- 0.005 #Distância entre pontos
grid <- expand.grid(X=seq(min(x),max(x),dis), Y=seq(min(y),max(y),dis)) |> 
    mutate(flag = def_pol(X,Y,contorno_cat)) |>  
  filter(flag) |> select(-flag)
gridded(grid) = ~ X + Y
plot(grid) 
points(x,y,col="red",pch=4)
```


```{r}
ko_variavel <- krige(formula=form, data_set_aux, grid, model=modelo, 
    block=c(0,0),
    nsim=0,
    na.action=na.pass,
    debug.level=-1,  
    )
```

Mapa de padrão espacial

```{r}
mapa <- as_tibble(ko_variavel) |> 
  ggplot(aes(x=X, y=Y)) + 
  geom_tile(aes(fill = var1.pred)) +
  # scale_fill_gradient(low = "yellow", high = "blue") + 
  scale_fill_viridis_c() +
  coord_equal() + 
  labs(fill=variavel,
       x="Longitude",
       y="Latitude")
mapa
ggsave(paste0("output/krigagem/krg-",ano_analise,"-",
                   unidade_analise,"-",variavel,".png"))
```


### Salvando o arquivo krigado

```{r}
df <- ko_variavel |> 
  as.tibble() |> 
  mutate(var1.var = sqrt(var1.var)) |> 
  rename(
    !!variavel := var1.pred,
    !!paste0(variavel,"_sd") := var1.var,
  )
write_rds(df,paste0("output/krigagem/estimativa-",ano_analise,"-",
                   unidade_analise,"-",variavel,".rds"))
```

### Separando o banco de dados para unidade POT e transformar se necessário

```{r}
ano_analise <- 2016
variavel <- "TCH"
unidade_analise <- "POT"
data_set_aux <- data_set |> 
  filter(
    ano == ano_analise,
    unidade == unidade_analise) |> 
  select(x,y,variavel) |> 
  rename(z = !!sym(variavel)) |> 
  group_by(x,y) |> 
  summarise(
    z = mean(z,na.rm = TRUE),
    .groups = "drop"
  )
glimpse(data_set_aux)
writexl::write_xlsx(data_set_aux,"data_set_aux.xlsx")
```

### Transformar se necessário

```{r}
coordinates(data_set_aux) = ~ x + y  
form <- z ~ 1 # fórmula da função variogram
```


Construir o semivariograma experimental

```{r}
vari_exp <- variogram(form, data = data_set_aux,
                      cressie = FALSE,
                      cutoff = 0.10, # distância máxima do semivariograma
                      width = .008) # distância entre pontos
vari_exp  |>  
 ggplot(aes(x=dist, y=gamma)) +
 geom_point() +
 labs(x="lag (º)",
      y=expression(paste(gamma,"(h)")))
```
## Escolher o melhor modelo

```{r}
patamar=600
alcance=0.02
epepita=0
modelo_1 <- fit.variogram(vari_exp,vgm(patamar,"Sph",alcance,epepita))
modelo_2 <- fit.variogram(vari_exp,vgm(patamar,"Exp",alcance,epepita))
modelo_3 <- fit.variogram(vari_exp,vgm(patamar,"Gau",alcance,epepita))
plot_my_models(modelo_1,modelo_2,modelo_3)
```
### Validação Cruzada
```{r}
my_cross_validation(data_set_aux,form,modelo_1,modelo_2,modelo_3)
```
### Escolha do melhor modelo

O melhor modelo é aquele que apresenta um coeficiente de regressão o mais próximo de $1$ e o intercepto o mais próximo de $0$, com o menor RMSE possível.

#### Definido o melhor modelo, precisamos guardar os valores.

```{r}
modelo <- modelo_2 ## sempre modificar
model <- modelo |> slice(2) |> pull(model)
rss <- round(attr(modelo, "SSErr"),4)
c0 <- round(modelo$psill[[1]],4)
c0_c1 <- round(sum(modelo$psill),4)
a <- ifelse(model == "Gau", round(modelo$range[[2]]*(3^.5),2),
            ifelse(model == "Exp",round(3*modelo$range[[2]],2),
            round(modelo$range[[2]],2)))
r2 <- vari_exp |> add_column( model = model, a=a, c0 = c0,
                                  c0_c1 = c0_c1) |>
    mutate(
      gamma_m = ifelse(model == "Sph",
        ifelse(dist <= a, c0 + (c0_c1 - c0) * (3/2 * (dist/a) - 1/2 * (dist/a)^3),c0_c1), ifelse(model == "Exp", c0 + (c0_c1-c0)*(1-exp(-3*(dist/a))),c0 + (c0_c1-c0)*(1-exp(-3*(dist/a)^2)))),
      residuo_total = (gamma-mean(gamma))^2,
      residuo_mod = (gamma - gamma_m)^2
    ) |>
    summarise(
      r2=(sum(residuo_total) - sum(residuo_mod))/sum(residuo_total)
    ) |> pull(r2)

tibble(
  ano_analise, unidade_analise, variavel, model, c0, c0_c1, a, rss, r2
) |> mutate(gde = c0/c0_c1, .after = "a") |>
  write_csv(paste0("output/best-fit/",ano_analise,"-",
                   unidade_analise,"-",variavel,".csv"))

ls_csv <- list.files("output/best-fit/",full.names = TRUE,pattern = ".csv")
map_df(ls_csv, read_csv) |>
  writexl::write_xlsx("output/semivariogram-models.xlsx")
png(filename = paste0("output/semivariogram-img/semivar-",
                      ano_analise,"-",
                   unidade_analise,"-",variavel,".png"),
    width = 800, height = 600)
plot(vari_exp,model=modelo,cex.lab=2, col=1,pl=F,pch=16,cex=2.2,ylab=list("Semivariância",cex=2.3),xlab=list("Distância de Separação h (m)",cex=2.3,cex.axis=4))
dev.off()
```

### Krigragem ordinária (KO)
Utilizando o algorítmo de KO, vamos estimar  nos locais não amostrados.


```{r}
x<-data_set_aux$x
y<-data_set_aux$y
dis <- 0.005 #Distância entre pontos
grid <- expand.grid(X=seq(min(x),max(x),dis), Y=seq(min(y),max(y),dis)) |> 
    mutate(flag = def_pol(X,Y,contorno_pot)) |>  
  filter(flag) |> select(-flag)
gridded(grid) = ~ X + Y
plot(grid) 
points(x,y,col="red",pch=4)
```


```{r}
ko_variavel <- krige(formula=form, data_set_aux, grid, model=modelo, 
    block=c(0,0),
    nsim=0,
    na.action=na.pass,
    debug.level=-1,  
    )
```

Mapa de padrão espacial

```{r}
mapa <- as_tibble(ko_variavel) |> 
  ggplot(aes(x=X, y=Y)) + 
  geom_tile(aes(fill = var1.pred)) +
  # scale_fill_gradient(low = "yellow", high = "blue") + 
  scale_fill_viridis_c() +
  coord_equal() + 
  labs(fill=variavel,
       x="Longitude",
       y="Latitude")
mapa
ggsave(paste0("output/krigagem/krg-",ano_analise,"-",
                   unidade_analise,"-",variavel,".png"))
```


### Salvando o arquivo krigado

```{r}
df <- ko_variavel |> 
  as.tibble() |> 
  mutate(var1.var = sqrt(var1.var)) |> 
  rename(
    !!variavel := var1.pred,
    !!paste0(variavel,"_sd") := var1.var,
  )
write_rds(df,paste0("output/krigagem/estimativa-",ano_analise,"-",
                   unidade_analise,"-",variavel,".rds"))
```
## 4. Aprendizado de máquina com validação final a partir dos dados de 2018 e por UNIDADE
